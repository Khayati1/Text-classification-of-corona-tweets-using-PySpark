{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 1\n",
    "Corona NLP Dataset"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Name: Khayati Chadha\n",
    "Roll No: 015033\n",
    "Class: BDA-01\n",
    "Section: G"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# used to create DataFrame\n",
    "from pyspark.sql import SparkSession "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#to construct SparkSession instances with app name corona\n",
    "spark = SparkSession.builder.appName('corona').getOrCreate() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reading the csv file \n",
    "data=spark.read.csv('Corona_NLP1.csv', header = True, inferSchema=True,sep= ',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------------+--------------------+----------+--------------------+\n",
      "|            UserName|          ScreenName|            Location|           Sentiment|   TweetAt|       OriginalTweet|\n",
      "+--------------------+--------------------+--------------------+--------------------+----------+--------------------+\n",
      "|                3799|               48751|              London|             Neutral|16-03-2020|@MeNyrbie @Phil_G...|\n",
      "|                3800|               48752|                  UK|            Positive|16-03-2020|advice Talk to yo...|\n",
      "|                3801|               48753|           Vagabonds|            Positive|16-03-2020|Coronavirus Austr...|\n",
      "|                3802|               48754|                null|            Positive|16-03-2020|My food stock is ...|\n",
      "|              PLEASE|         don't panic| THERE WILL BE EN...|                null|      null|                null|\n",
      "|           Stay calm|          stay safe.|                null|                null|      null|                null|\n",
      "|#COVID19france #C...|                null|                null|                null|      null|                null|\n",
      "|                3803|               48755|                null|  Extremely Negative|16-03-2020|Me, ready to go a...|\n",
      "|Not because I'm p...| but because my f...|          but please| don't panic. It ...|      null|                null|\n",
      "|#CoronavirusFranc...|                null|                null|                null|      null|                null|\n",
      "|                3804|               48756|ÃœT: 36.319708,-82...|            Positive|16-03-2020|As news of the re...|\n",
      "|                3805|               48757|35.926541,-78.753267|            Positive|16-03-2020|\"Cashier at groce...|\n",
      "|                3806|               48758|             Austria|             Neutral|16-03-2020|Was at the superm...|\n",
      "|#toiletpapercrisi...|                null|                null|                null|      null|                null|\n",
      "|                3807|               48759|     Atlanta, GA USA|            Positive|16-03-2020|Due to COVID-19 o...|\n",
      "|                3808|               48760|    BHAVNAGAR,GUJRAT|            Negative|16-03-2020|For corona preven...|\n",
      "|                3809|               48761|      Makati, Manila|             Neutral|16-03-2020|All month there h...|\n",
      "|                3810|               48762|Pitt Meadows, BC,...|  Extremely Positive|16-03-2020|Due to the Covid-...|\n",
      "|The wait time may...| particularly bee...|                null|                null|      null|                null|\n",
      "|We thank you for ...|                null|                null|                null|      null|                null|\n",
      "+--------------------+--------------------+--------------------+--------------------+----------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#viewing the dataset\n",
    "data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- UserName: string (nullable = true)\n",
      " |-- ScreenName: string (nullable = true)\n",
      " |-- Location: string (nullable = true)\n",
      " |-- Sentiment: string (nullable = true)\n",
      " |-- TweetAt: string (nullable = true)\n",
      " |-- OriginalTweet: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#to print out schema in tree format\n",
    "data.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['UserName', 'ScreenName', 'Location', 'Sentiment', 'TweetAt', 'OriginalTweet']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#listing down the columns of the data\n",
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "65074\n"
     ]
    }
   ],
   "source": [
    "#dropping the duplicate values\n",
    "data = data.dropDuplicates()\n",
    "print(data.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32621\n"
     ]
    }
   ],
   "source": [
    "#dropping null values\n",
    "data = data.na.drop()\n",
    "print(data.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#defining the list of different sentiments\n",
    "sentiments = ['Positive','Negative','Neutral','Extremely Positive','Extremely Negative']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#filtering out data in the dataset\n",
    "data = data.filter(data.Sentiment.isin(sentiments))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#counting the distinct sentiments in the dataset\n",
    "data.select('Sentiment').distinct().count()\n",
    "#there are 5 distinct sentiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+\n",
      "|         Sentiment|\n",
      "+------------------+\n",
      "|Extremely Negative|\n",
      "|           Neutral|\n",
      "|          Positive|\n",
      "|          Negative|\n",
      "|Extremely Positive|\n",
      "+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#showing the distinct sentiments in the dataset\n",
    "data.select('Sentiment').distinct().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Computes the character length of string data or number of bytes of binary data\n",
    "from pyspark.sql.functions import length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Viewing the length of each and every tweet\n",
    "data=data.withColumn('length', length(data['OriginalTweet']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------+--------------------+------------------+----------+--------------------+------+\n",
      "|UserName|ScreenName|            Location|         Sentiment|   TweetAt|       OriginalTweet|length|\n",
      "+--------+----------+--------------------+------------------+----------+--------------------+------+\n",
      "|    3926|     48878| ????? ???? ????????|          Negative|16-03-2020|#unpopularopinion...|   175|\n",
      "|    4155|     49107|      Owensboro, KY |           Neutral|16-03-2020|Just online shopp...|    80|\n",
      "|    4247|     49199|            New York|          Positive|16-03-2020|I know a lot of g...|   269|\n",
      "|    4949|     49901|         Houston, TX|          Positive|17-03-2020|Our latest issue ...|   164|\n",
      "|    5065|     50017|  Manchester, Europe|Extremely Positive|17-03-2020|If you are health...|   202|\n",
      "|    5322|     50274|      Leeds, England|          Positive|17-03-2020|#COVID2019 local ...|   191|\n",
      "|    5766|     50718|          upstate NY|          Negative|17-03-2020|Seeing those empt...|   181|\n",
      "|    6180|     51132|          Texas, USA|          Positive|17-03-2020|Governor @GregAbb...|   120|\n",
      "|    6209|     51161|          Dallas, TX|           Neutral|17-03-2020|U.S. retail sales...|   182|\n",
      "|    6270|     51222|        Georgia, USA|          Negative|17-03-2020|So no gatherings ...|   192|\n",
      "|    6290|     51242|    Williamsport, MD|          Negative|17-03-2020|This #coronavirus...|   193|\n",
      "|    6328|     51280|        The Islands |           Neutral|17-03-2020|What about all of...|   274|\n",
      "|    6333|     51285|        St.Louis, MO|Extremely Negative|17-03-2020|Next time an idio...|   160|\n",
      "|    6359|     51311|     Los Angeles, CA|          Positive|17-03-2020|An interesting lo...|    73|\n",
      "|    6735|     51687|Salt Lake City, Utah|           Neutral|18-03-2020|LISTEN: @abc4utah...|    24|\n",
      "|    6798|     51750|        New York, NY|Extremely Positive|18-03-2020|Practicing social...|   213|\n",
      "|    7246|     52198|         Philippines|           Neutral|18-03-2020|Lazada, Shopee, a...|    84|\n",
      "|    7399|     52351|    Washington, D.C.|Extremely Positive|18-03-2020|'Our absolute goa...|   131|\n",
      "|    7890|     52842|     Bakersfield, CA|           Neutral|18-03-2020|Police said they ...|   172|\n",
      "|    8349|     53301|       Working on it|           Neutral|18-03-2020|Me Vs #coronaviru...|    83|\n",
      "+--------+----------+--------------------+------------------+----------+--------------------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Viewing the length of each and every tweet\n",
    "data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#renaming the column\n",
    "data=data.withColumnRenamed(\"Sentiment\",\"sentiment\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+------------------+\n",
      "|         Sentiment|       avg(length)|\n",
      "+------------------+------------------+\n",
      "|Extremely Negative|179.08476571697668|\n",
      "|           Neutral|134.06076810889644|\n",
      "|          Positive| 167.5731693929081|\n",
      "|          Negative|165.74478227261014|\n",
      "|Extremely Positive|183.49146433990896|\n",
      "+------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#calculating the mean length of different sentiments\n",
    "data.groupby('Sentiment').mean().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Performing tokenization to divide the entire corpus of text into vords, removing the stop words which are \n",
    "#basically unnecessary and irrelevant, using count vectorizer, inverse document frequency and string indexer to\n",
    "#transform the text\n",
    "from pyspark.ml.feature import Tokenizer, StopWordsRemover, CountVectorizer, IDF, StringIndexer\n",
    "\n",
    "tokenizer=Tokenizer(inputCol=\"OriginalTweet\", outputCol=\"token_text\")\n",
    "stopremove=StopWordsRemover(inputCol=\"token_text\", outputCol=\"stop_tokens\")\n",
    "count_vec=CountVectorizer(inputCol=\"stop_tokens\", outputCol=\"c_vec\")\n",
    "idf=IDF(inputCol=\"c_vec\", outputCol=\"tf_idf\")\n",
    "\n",
    "# convert the labels in numbers\n",
    "label_to_num = StringIndexer(inputCol=\"sentiment\", outputCol='label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#feature transformer that merges multiple columns into a vector column.\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.linalg import Vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned = VectorAssembler(inputCols=['tf_idf','length'], outputCol='features')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import NaiveBayes, RandomForestClassifier, DecisionTreeClassifier\n",
    "#instantitating naivesbayes, randomforest classifier and decision tree classifier\n",
    "nb=NaiveBayes()\n",
    "rf=RandomForestClassifier()\n",
    "dtc=DecisionTreeClassifier()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "#building a pieline for pre-processing the text \n",
    "data_prep_pipeline= Pipeline(stages=[label_to_num, tokenizer, stopremove,count_vec, idf,cleaned])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fitting the model on the data\n",
    "cleaned_data= data_prep_pipeline.fit(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#transforming the data\n",
    "cleaned_data=cleaned_data.transform(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------+--------------------+------------------+----------+--------------------+------+-----+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|UserName|ScreenName|            Location|         sentiment|   TweetAt|       OriginalTweet|length|label|          token_text|         stop_tokens|               c_vec|              tf_idf|            features|\n",
      "+--------+----------+--------------------+------------------+----------+--------------------+------+-----+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|    3926|     48878| ????? ???? ????????|          Negative|16-03-2020|#unpopularopinion...|   175|  1.0|[#unpopularopinio...|[#unpopularopinio...|(80619,[5,56,60,8...|(80619,[5,56,60,8...|(80620,[5,56,60,8...|\n",
      "|    4155|     49107|      Owensboro, KY |           Neutral|16-03-2020|Just online shopp...|    80|  2.0|[just, online, sh...|[online, shopping...|(80619,[6,13,14,8...|(80619,[6,13,14,8...|(80620,[6,13,14,8...|\n",
      "|    4247|     49199|            New York|          Positive|16-03-2020|I know a lot of g...|   269|  0.0|[i, know, a, lot,...|[know, lot, groce...|(80619,[0,3,7,17,...|(80619,[0,3,7,17,...|(80620,[0,3,7,17,...|\n",
      "|    4949|     49901|         Houston, TX|          Positive|17-03-2020|Our latest issue ...|   164|  0.0|[our, latest, iss...|[latest, issue, e...|(80619,[2,10,15,3...|(80619,[2,10,15,3...|(80620,[2,10,15,3...|\n",
      "|    5065|     50017|  Manchester, Europe|Extremely Positive|17-03-2020|If you are health...|   202|  3.0|[if, you, are, he...|[healthy,, risk, ...|(80619,[0,5,6,17,...|(80619,[0,5,6,17,...|(80620,[0,5,6,17,...|\n",
      "|    5322|     50274|      Leeds, England|          Positive|17-03-2020|#COVID2019 local ...|   191|  0.0|[#covid2019, loca...|[#covid2019, loca...|(80619,[5,16,19,4...|(80619,[5,16,19,4...|(80620,[5,16,19,4...|\n",
      "|    5766|     50718|          upstate NY|          Negative|17-03-2020|Seeing those empt...|   181|  1.0|[seeing, those, e...|[seeing, empty, s...|(80619,[3,7,16,56...|(80619,[3,7,16,56...|(80620,[3,7,16,56...|\n",
      "|    6180|     51132|          Texas, USA|          Positive|17-03-2020|Governor @GregAbb...|   120|  0.0|[governor, @grega...|[governor, @grega...|(80619,[38,45,116...|(80619,[38,45,116...|(80620,[38,45,116...|\n",
      "|    6209|     51161|          Dallas, TX|           Neutral|17-03-2020|U.S. retail sales...|   182|  2.0|[u.s., retail, sa...|[u.s., retail, sa...|(80619,[9,59,91,2...|(80619,[9,59,91,2...|(80620,[9,59,91,2...|\n",
      "|    6270|     51222|        Georgia, USA|          Negative|17-03-2020|So no gatherings ...|   192|  1.0|[so, no, gatherin...|[gatherings, 10, ...|(80619,[3,7,45,56...|(80619,[3,7,45,56...|(80620,[3,7,45,56...|\n",
      "|    6290|     51242|    Williamsport, MD|          Negative|17-03-2020|This #coronavirus...|   193|  1.0|[this, #coronavir...|[#coronavirus, sc...|(80619,[0,7,47,59...|(80619,[0,7,47,59...|(80620,[0,7,47,59...|\n",
      "|    6328|     51280|        The Islands |           Neutral|17-03-2020|What about all of...|   274|  2.0|[what, about, all...|[us, cash, regist...|(80619,[0,3,8,12,...|(80619,[0,3,8,12,...|(80620,[0,3,8,12,...|\n",
      "|    6333|     51285|        St.Louis, MO|Extremely Negative|17-03-2020|Next time an idio...|   160|  4.0|[next, time, an, ...|[next, time, idio...|(80619,[0,3,7,17,...|(80619,[0,3,7,17,...|(80620,[0,3,7,17,...|\n",
      "|    6359|     51311|     Los Angeles, CA|          Positive|17-03-2020|An interesting lo...|    73|  0.0|[an, interesting,...|[interesting, loo...|(80619,[0,13,14,1...|(80619,[0,13,14,1...|(80620,[0,13,14,1...|\n",
      "|    6735|     51687|Salt Lake City, Utah|           Neutral|18-03-2020|LISTEN: @abc4utah...|    24|  2.0|[listen:, @abc4ut...|[listen:, @abc4ut...|(80619,[27806,506...|(80619,[27806,506...|(80620,[27806,506...|\n",
      "|    6798|     51750|        New York, NY|Extremely Positive|18-03-2020|Practicing social...|   213|  3.0|[practicing, soci...|[practicing, soci...|(80619,[1,6,11,36...|(80619,[1,6,11,36...|(80620,[1,6,11,36...|\n",
      "|    7246|     52198|         Philippines|           Neutral|18-03-2020|Lazada, Shopee, a...|    84|  2.0|[lazada,, shopee,...|[lazada,, shopee,...|(80619,[6,665,118...|(80619,[6,665,118...|(80620,[6,665,118...|\n",
      "|    7399|     52351|    Washington, D.C.|Extremely Positive|18-03-2020|'Our absolute goa...|   131|  3.0|['our, absolute, ...|['our, absolute, ...|(80619,[4,6,51,54...|(80619,[4,6,51,54...|(80620,[4,6,51,54...|\n",
      "|    7890|     52842|     Bakersfield, CA|           Neutral|18-03-2020|Police said they ...|   172|  2.0|[police, said, th...|[police, said, re...|(80619,[3,7,8,31,...|(80619,[3,7,8,31,...|(80620,[3,7,8,31,...|\n",
      "|    8349|     53301|       Working on it|           Neutral|18-03-2020|Me Vs #coronaviru...|    83|  2.0|[me, vs, #coronav...|[vs, #coronavirus...|(80619,[0,4,21,36...|(80619,[0,4,21,36...|(80620,[0,4,21,36...|\n",
      "+--------+----------+--------------------+------------------+----------+--------------------+------+-----+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#viewing the cleaned data \n",
    "cleaned_data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#extracting only label and features \n",
    "cleaned_data=cleaned_data.select(['label', 'features'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+\n",
      "|label|            features|\n",
      "+-----+--------------------+\n",
      "|  1.0|(80620,[5,56,60,8...|\n",
      "|  2.0|(80620,[6,13,14,8...|\n",
      "|  0.0|(80620,[0,3,7,17,...|\n",
      "|  0.0|(80620,[2,10,15,3...|\n",
      "|  3.0|(80620,[0,5,6,17,...|\n",
      "|  0.0|(80620,[5,16,19,4...|\n",
      "|  1.0|(80620,[3,7,16,56...|\n",
      "|  0.0|(80620,[38,45,116...|\n",
      "|  2.0|(80620,[9,59,91,2...|\n",
      "|  1.0|(80620,[3,7,45,56...|\n",
      "|  1.0|(80620,[0,7,47,59...|\n",
      "|  2.0|(80620,[0,3,8,12,...|\n",
      "|  4.0|(80620,[0,3,7,17,...|\n",
      "|  0.0|(80620,[0,13,14,1...|\n",
      "|  2.0|(80620,[27806,506...|\n",
      "|  3.0|(80620,[1,6,11,36...|\n",
      "|  2.0|(80620,[6,665,118...|\n",
      "|  3.0|(80620,[4,6,51,54...|\n",
      "|  2.0|(80620,[3,7,8,31,...|\n",
      "|  2.0|(80620,[0,4,21,36...|\n",
      "+-----+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#viewing the selected columns\n",
    "cleaned_data.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ML Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Spilting the data into train and test\n",
    "(training, testing)=cleaned_data.randomSplit([0.7,0.3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naives Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fitting the model on the dataset\n",
    "spam_predictor_nb=nb.fit(training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#transforming the model\n",
    "test_results_nb=spam_predictor_nb.transform(testing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+--------------------+--------------------+----------+\n",
      "|label|            features|       rawPrediction|         probability|prediction|\n",
      "+-----+--------------------+--------------------+--------------------+----------+\n",
      "|  0.0|(80620,[0,1,3,7,6...|[-1972.4818762147...|[0.99996698630983...|       0.0|\n",
      "|  0.0|(80620,[0,2,12,70...|[-2064.7263066475...|[1.13313039964965...|       1.0|\n",
      "|  0.0|(80620,[0,3,12,21...|[-1365.0355758765...|[0.99990741851604...|       0.0|\n",
      "|  0.0|(80620,[0,5,8,23,...|[-885.90026777626...|[1.70549320586324...|       1.0|\n",
      "|  0.0|(80620,[0,9,12,10...|[-2180.2237815626...|[1.62538794124251...|       4.0|\n",
      "|  0.0|(80620,[0,9,12,12...|[-1455.4884940237...|[0.99999999999958...|       0.0|\n",
      "|  0.0|(80620,[0,12,24,3...|[-1484.0008205774...|[1.40300473694068...|       3.0|\n",
      "|  0.0|(80620,[0,13,14,1...|[-289.77611252761...|[0.98033632604683...|       0.0|\n",
      "|  0.0|(80620,[0,13,46,8...|[-1774.0938598433...|[0.03009188542530...|       1.0|\n",
      "|  0.0|(80620,[0,22,26,3...|[-2248.6069232685...|[0.99689332090925...|       0.0|\n",
      "|  0.0|(80620,[1,10,15,5...|[-652.25028606384...|[1.76046383906974...|       1.0|\n",
      "|  0.0|(80620,[2,10,15,3...|[-806.95852828654...|[3.38319105216317...|       2.0|\n",
      "|  0.0|(80620,[2,71,131,...|[-1147.5728609069...|[1.10631310097823...|       2.0|\n",
      "|  0.0|(80620,[3,7,10,34...|[-2050.7398957609...|[3.44789836864863...|       1.0|\n",
      "|  0.0|(80620,[5,6,16,31...|[-1532.9743733701...|[8.89460967924193...|       1.0|\n",
      "|  0.0|(80620,[5,16,19,4...|[-949.42392010909...|[0.00251779992131...|       1.0|\n",
      "|  0.0|(80620,[6,7,48,92...|[-676.56725390225...|[0.98975721762590...|       0.0|\n",
      "|  0.0|(80620,[6,11,13,1...|[-1809.8702208216...|[1.0,2.1298491159...|       0.0|\n",
      "|  0.0|(80620,[6,28,33,2...|[-2065.4839394744...|[0.99953661280043...|       0.0|\n",
      "|  0.0|(80620,[38,45,116...|[-764.36081729215...|[2.36299670089327...|       3.0|\n",
      "+-----+--------------------+--------------------+--------------------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#viewing the results of the test data\n",
    "test_results_nb.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#used for multi classification problem\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set accuracy = 0.3827981182245858\n"
     ]
    }
   ],
   "source": [
    "#evaluating the accuracy of the model\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\",\n",
    " metricName=\"accuracy\")\n",
    "accuracy = evaluator.evaluate(test_results_nb)\n",
    "print(\"Test set accuracy = \" + str(accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fitting the model on the dataset\n",
    "spam_predictor=dtc.fit(training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#transforming the model\n",
    "test_results=spam_predictor.transform(testing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+--------------------+--------------------+----------+\n",
      "|label|            features|       rawPrediction|         probability|prediction|\n",
      "+-----+--------------------+--------------------+--------------------+----------+\n",
      "|  0.0|(80620,[0,1,3,7,6...|[3102.0,2474.0,14...|[0.30219191427179...|       0.0|\n",
      "|  0.0|(80620,[0,2,12,70...|[3102.0,2474.0,14...|[0.30219191427179...|       0.0|\n",
      "|  0.0|(80620,[0,3,12,21...|[3102.0,2474.0,14...|[0.30219191427179...|       0.0|\n",
      "|  0.0|(80620,[0,5,8,23,...|[38.0,202.0,11.0,...|[0.08920187793427...|       1.0|\n",
      "|  0.0|(80620,[0,9,12,10...|[3102.0,2474.0,14...|[0.30219191427179...|       0.0|\n",
      "|  0.0|(80620,[0,9,12,12...|[3102.0,2474.0,14...|[0.30219191427179...|       0.0|\n",
      "|  0.0|(80620,[0,12,24,3...|[201.0,22.0,12.0,...|[0.55988857938718...|       0.0|\n",
      "|  0.0|(80620,[0,13,14,1...|[2465.0,2274.0,28...|[0.25671735055196...|       2.0|\n",
      "|  0.0|(80620,[0,13,46,8...|[3102.0,2474.0,14...|[0.30219191427179...|       0.0|\n",
      "|  0.0|(80620,[0,22,26,3...|[3102.0,2474.0,14...|[0.30219191427179...|       0.0|\n",
      "|  0.0|(80620,[1,10,15,5...|[2465.0,2274.0,28...|[0.25671735055196...|       2.0|\n",
      "|  0.0|(80620,[2,10,15,3...|[3102.0,2474.0,14...|[0.30219191427179...|       0.0|\n",
      "|  0.0|(80620,[2,71,131,...|[2465.0,2274.0,28...|[0.25671735055196...|       2.0|\n",
      "|  0.0|(80620,[3,7,10,34...|[3102.0,2474.0,14...|[0.30219191427179...|       0.0|\n",
      "|  0.0|(80620,[5,6,16,31...|[3102.0,2474.0,14...|[0.30219191427179...|       0.0|\n",
      "|  0.0|(80620,[5,16,19,4...|[3102.0,2474.0,14...|[0.30219191427179...|       0.0|\n",
      "|  0.0|(80620,[6,7,48,92...|[2465.0,2274.0,28...|[0.25671735055196...|       2.0|\n",
      "|  0.0|(80620,[6,11,13,1...|[3102.0,2474.0,14...|[0.30219191427179...|       0.0|\n",
      "|  0.0|(80620,[6,28,33,2...|[3102.0,2474.0,14...|[0.30219191427179...|       0.0|\n",
      "|  0.0|(80620,[38,45,116...|[2465.0,2274.0,28...|[0.25671735055196...|       2.0|\n",
      "+-----+--------------------+--------------------+--------------------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#viewing the results of the test data\n",
    "test_results.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#instantiating the classfication evaluator\n",
    "acc_eval=MulticlassClassificationEvaluator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#evaluating the accuracy of the model\n",
    "acc=acc_eval.evaluate(test_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the model is:: 0.27065937557048814\n"
     ]
    }
   ],
   "source": [
    "print (\"Accuracy of the model is::\", acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fitting the model on the dataset\n",
    "spam_predictor_rf=rf.fit(training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "#transforming the model\n",
    "test_results_rf=spam_predictor_rf.transform(testing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+--------------------+--------------------+----------+\n",
      "|label|            features|       rawPrediction|         probability|prediction|\n",
      "+-----+--------------------+--------------------+--------------------+----------+\n",
      "|  0.0|(80620,[0,1,3,7,6...|[5.60104606499630...|[0.28005230324981...|       0.0|\n",
      "|  0.0|(80620,[0,2,12,70...|[5.53855401644105...|[0.27692770082205...|       0.0|\n",
      "|  0.0|(80620,[0,3,12,21...|[5.69880669421629...|[0.28494033471081...|       0.0|\n",
      "|  0.0|(80620,[0,5,8,23,...|[5.56385121263016...|[0.27819256063150...|       0.0|\n",
      "|  0.0|(80620,[0,9,12,10...|[5.36301256333227...|[0.26815062816661...|       0.0|\n",
      "|  0.0|(80620,[0,9,12,12...|[5.55481383646610...|[0.27774069182330...|       0.0|\n",
      "|  0.0|(80620,[0,12,24,3...|[5.6518393691567,...|[0.28259196845783...|       0.0|\n",
      "|  0.0|(80620,[0,13,14,1...|[5.55481383646610...|[0.27774069182330...|       0.0|\n",
      "|  0.0|(80620,[0,13,46,8...|[5.46099572704180...|[0.27304978635209...|       0.0|\n",
      "|  0.0|(80620,[0,22,26,3...|[5.55481383646610...|[0.27774069182330...|       0.0|\n",
      "|  0.0|(80620,[1,10,15,5...|[5.55481383646610...|[0.27774069182330...|       0.0|\n",
      "|  0.0|(80620,[2,10,15,3...|[5.48488913272779...|[0.27424445663638...|       0.0|\n",
      "|  0.0|(80620,[2,71,131,...|[5.53855401644105...|[0.27692770082205...|       0.0|\n",
      "|  0.0|(80620,[3,7,10,34...|[5.59200868883225...|[0.27960043444161...|       0.0|\n",
      "|  0.0|(80620,[5,6,16,31...|[5.55481383646610...|[0.27774069182330...|       0.0|\n",
      "|  0.0|(80620,[5,16,19,4...|[5.55587609102092...|[0.27779380455104...|       0.0|\n",
      "|  0.0|(80620,[6,7,48,92...|[5.59200868883225...|[0.27960043444161...|       0.0|\n",
      "|  0.0|(80620,[6,11,13,1...|[5.55481383646610...|[0.27774069182330...|       0.0|\n",
      "|  0.0|(80620,[6,28,33,2...|[5.55481383646610...|[0.27774069182330...|       0.0|\n",
      "|  0.0|(80620,[38,45,116...|[5.55481383646610...|[0.27774069182330...|       0.0|\n",
      "+-----+--------------------+--------------------+--------------------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#viewing the results of the test data\n",
    "test_results_rf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set accuracy = 0.28523215381468603\n"
     ]
    }
   ],
   "source": [
    "#evaluating the accuracy of the model\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\",\n",
    " metricName=\"accuracy\")\n",
    "accuracy_rf = evaluator.evaluate(test_results_rf)\n",
    "print(\"Test set accuracy = \" + str(accuracy_rf))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "model_errors= pd.DataFrame({\n",
    "    \"Model\": [\"DecisionTreeClassifier\",\"RandomForestClassifier\", \"NaivesBayes\"],\n",
    "    \"Score\": [acc, accuracy,accuracy_rf]\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>DecisionTreeClassifier</td>\n",
       "      <td>0.270659</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NaivesBayes</td>\n",
       "      <td>0.285232</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>RandomForestClassifier</td>\n",
       "      <td>0.382798</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    Model     Score\n",
       "0  DecisionTreeClassifier  0.270659\n",
       "2             NaivesBayes  0.285232\n",
       "1  RandomForestClassifier  0.382798"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_errors.sort_values(by='Score',ascending=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random Forest classifier is giving the best accuracy hence, I have selected random forest"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
